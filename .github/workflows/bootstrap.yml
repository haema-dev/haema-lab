name: Kubespray K8s

on:
  workflow_dispatch:
  push:
    branches: [ master ]
    paths:
      - '.github/workflows/bootstrap.yml'

jobs:
  setup:
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - uses: actions/checkout@v3

      - name: Connect to Tailscale
        uses: tailscale/github-action@v2
        with:
          version: 1.94.1
          authkey: ${{ secrets.TAILSCALE_AUTH_KEY }}

      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # Phase 1: Cleanup (FIRST_USERNAME)
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - name: ðŸ§¹ Cleanup K8s
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.PROXMOX_VM_TAILSCALE_IP }}
          username: ${{ secrets.FIRST_USERNAME }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          script: |
            set -e

            echo "ðŸ§¹ Cleanup Phase (Enhanced)"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

            USERNAME="${{ secrets.USERNAME }}"

            # 1. User shells restore
            echo "ðŸ”‘ Restoring user shells..."
            sudo usermod -s /bin/bash ${{ secrets.FIRST_USERNAME }} || true
            sudo usermod -s /bin/bash "$USERNAME" || true

            # 2. ëª¨ë“  ì„œë¹„ìŠ¤/í”„ë¡œì„¸ìŠ¤ ì¤‘ì§€
            echo "ðŸ›‘ Stopping all services..."
            sudo systemctl stop kubelet 2>/dev/null || true
            sudo systemctl stop etcd 2>/dev/null || true
            sudo systemctl stop containerd 2>/dev/null || true
            sudo systemctl disable kubelet 2>/dev/null || true
            sudo systemctl disable etcd 2>/dev/null || true

            sleep 2

            # 3. í”„ë¡œì„¸ìŠ¤ ê°•ì œ ì¢…ë£Œ
            echo "ðŸ”ª Killing processes..."
            for proc in kubelet apiserver etcd kube-controller-manager kube-scheduler kube-proxy coredns etcd-io containerd; do
              sudo pkill -9 $proc 2>/dev/null || true
            done

            sleep 2

            # 4. Docker/containerd ì •ë¦¬
            echo "ðŸ—‘ï¸  Cleaning containers..."
            if command -v docker &> /dev/null; then
              sudo docker ps -aq 2>/dev/null | xargs -r sudo docker rm -f 2>/dev/null || true
            fi
            if command -v crictl &> /dev/null; then
              sudo crictl ps -a -q 2>/dev/null | xargs -r sudo crictl rm 2>/dev/null || true
            fi

            # 5. Kubernetes reset
            echo "ðŸ”„ Resetting Kubernetes..."
            sudo kubeadm reset --force 2>/dev/null || true

            sleep 2

            # 6. ðŸ†• ê°•ì œ ë””ë ‰í† ë¦¬ ì •ë¦¬ (ìž¬ê·€ + force)
            echo "ðŸ—‚ï¸  Force cleaning directories..."
            sudo rm -rf --force /etc/kubernetes 2>/dev/null || true
            sudo rm -rf --force /var/lib/kubelet 2>/dev/null || true
            sudo rm -rf --force /var/lib/etcd 2>/dev/null || true
            sudo rm -rf --force /var/lib/cni 2>/dev/null || true
            sudo rm -rf --force /etc/cni/net.d 2>/dev/null || true
            sudo rm -rf --force /var/lib/calico 2>/dev/null || true

            # 7. ðŸ†• containerd ìºì‹œ ì •ë¦¬
            echo "ðŸ§¹ Cleaning containerd..."
            sudo systemctl stop containerd 2>/dev/null || true
            sudo pkill -9 containerd 2>/dev/null || true
            sudo rm -rf --force /var/lib/containerd 2>/dev/null || true
            sudo rm -rf --force /run/containerd 2>/dev/null || true

            sleep 2

            # 8. ðŸ†• Kubelet ë°ì´í„° ì •ë¦¬
            echo "ðŸ—‘ï¸  Cleaning kubelet data..."
            sudo rm -rf --force /var/lib/kubelet/pods 2>/dev/null || true
            sudo rm -rf --force /var/lib/kubelet/pod_resources 2>/dev/null || true
            sudo rm -rf --force /var/lib/kubelet/device-plugins 2>/dev/null || true
            sudo rm -rf --force /var/lib/kubelet/kubeadm-flags.env 2>/dev/null || true
            sudo rm -rf --force /var/lib/kubelet/config.yaml 2>/dev/null || true

            # 9. Kubespray cleanup
            echo "ðŸ“¦ Cleaning Kubespray..."
            [ -d /opt/kubespray ] && sudo rm -rf /opt/kubespray
            [ -d /opt/venv-kubespray ] && sudo rm -rf /opt/venv-kubespray

            # 10. Home directories cleanup
            echo "ðŸ  Cleaning home directories..."
            sudo rm -rf /home/$USERNAME/.kube 2>/dev/null || true
            sudo rm -rf /home/$USERNAME/.ansible 2>/dev/null || true
            sudo rm -rf /home/$USERNAME/.cache/pip 2>/dev/null || true

            # 11. Prepare /opt
            echo "ðŸ“ Preparing /opt..."
            sudo mkdir -p /opt
            sudo chown -R "$USERNAME:$USERNAME" /opt

            # 12. Configure sudoers
            echo "ðŸ” Configuring sudoers..."
            if ! sudo grep -q "^$USERNAME ALL=(ALL) NOPASSWD: ALL" /etc/sudoers.d/$USERNAME 2>/dev/null; then
              echo "$USERNAME ALL=(ALL) NOPASSWD: ALL" | sudo tee /etc/sudoers.d/$USERNAME > /dev/null
              sudo chmod 440 /etc/sudoers.d/$USERNAME
            fi

            # 13. ðŸ†• í¬íŠ¸ ì •ë¦¬ (ìž¬ì‹œë„)
            echo "ðŸ”Œ Cleaning ports (with retry)..."
            max_retries=5
            for attempt in $(seq 1 $max_retries); do
              # í¬íŠ¸ ê°•ì œ í•´ì œ
              for port in 6443 2379 10250 10259 10257 8080; do
                sudo fuser -k -9 $port/tcp 2>/dev/null || true
              done
              
              # í¬íŠ¸ ìƒíƒœ í™•ì¸
              ports_in_use=$(sudo netstat -tlnp 2>/dev/null | grep -E '6443|10259|10257' | wc -l || echo 0)
              if [ $ports_in_use -eq 0 ]; then
                echo "âœ… All ports free (attempt $attempt/$max_retries)"
                break
              fi
              
              echo "âš ï¸  Ports still in use, retrying... ($attempt/$max_retries)"
              sleep 2
            done

            # 14. ðŸ†• ê²€ì¦: íŒŒì¼ í™•ì¸
            echo "ðŸ” Verifying cleanup..."
            if [ ! -d /etc/kubernetes ]; then
              echo "   âœ… /etc/kubernetes removed"
            else
              echo "   âŒ /etc/kubernetes still exists!"
              ls -la /etc/kubernetes/
              sudo rm -rf /etc/kubernetes
            fi

            # 15. Hostname ì„¤ì •
            echo "ðŸ·ï¸  Setting hostname..."
            sudo hostnamectl set-hostname "${{ secrets.USERNAME }}"

            sleep 3
            echo "âœ… Enhanced Cleanup complete"

      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # Phase 2: Setup (USERNAME)
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - name: Setup K8s with Kubespray
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.PROXMOX_VM_TAILSCALE_IP }}
          username: ${{ secrets.USERNAME }}
          key: ${{ secrets.SSH_USERNAME_PRIVATE_KEY }}
          script: |
            set -e

            echo "ðŸš€ Setup Phase"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

            KUBE_VERSION="1.31.6"
            KUBESPRAY_VERSION="v2.29.0"
            KUBESPRAY_HOME="/opt/kubespray"
            VENV_PATH="/opt/venv-kubespray"
            HOST_IP="${{ secrets.HOST_IP }}"
            USERNAME="${{ secrets.USERNAME }}"


            # 1ï¸âƒ£ Cluster health check
            echo ""
            echo "ðŸ“Š Checking cluster health..."

            check_cluster_health() {
              if ! command -v kubectl &> /dev/null; then
                return 1
              fi
              
              if ! kubectl cluster-info &>/dev/null; then
                return 1
              fi
              
              READY_NODES=$(kubectl get nodes --no-headers 2>/dev/null | grep -c " Ready " || echo 0)
              TOTAL_NODES=$(kubectl get nodes --no-headers 2>/dev/null | wc -l || echo 0)
              
              if [ "$READY_NODES" -eq "$TOTAL_NODES" ] && [ "$TOTAL_NODES" -gt 0 ]; then
                return 0
              fi
              return 1
            }

            if check_cluster_health; then
              echo "âœ… Cluster is healthy - skipping setup"
              kubectl get nodes
              exit 0
            fi

            echo "âš ï¸  Cluster not ready, proceeding..."

            # 2ï¸âƒ£ Install dependencies
            echo ""
            echo "ðŸ“¦ Installing dependencies..."
            sudo apt-get update -qq
            sudo apt-get install -y -qq \
              python3-pip python3-venv git jq curl \
              2>&1 | grep -v "^Processing\|^done" || true

            # 3ï¸âƒ£ Setup Kubespray (fixed: removed sudo)
            echo ""
            echo "ðŸ“¥ Setting up Kubespray..."

            if [ -d "$KUBESPRAY_HOME/.git" ]; then
              CURRENT_VERSION=$(cd "$KUBESPRAY_HOME" && git describe --tags 2>/dev/null || echo "unknown")
              if [ "$CURRENT_VERSION" = "$KUBESPRAY_VERSION" ]; then
                echo "   âœ… Kubespray $KUBESPRAY_VERSION already cloned"
              else
                echo "   ðŸ”„ Updating from $CURRENT_VERSION to $KUBESPRAY_VERSION"
                cd "$KUBESPRAY_HOME"
                git fetch origin
                git checkout "$KUBESPRAY_VERSION"
              fi
            else
              echo "   ðŸ“¥ Cloning Kubespray..."
              git clone --depth 1 --branch "$KUBESPRAY_VERSION" \
                https://github.com/kubernetes-sigs/kubespray.git "$KUBESPRAY_HOME"
            fi

            # 4ï¸âƒ£ Python environment
            echo ""
            echo "ðŸ Setting up Python environment..."

            if [ ! -d "$VENV_PATH" ]; then
              python3 -m venv "$VENV_PATH"
            fi

            source "$VENV_PATH/bin/activate"
            pip install -q --upgrade pip
            pip install -q -r "$KUBESPRAY_HOME/requirements.txt"

            # 5ï¸âƒ£ Ansible inventory (fixed: removed sudo mkdir, fixed YAML indentation)
            echo ""
            echo "ðŸ“‹ Configuring Ansible inventory..."

            INVENTORY_DIR="$KUBESPRAY_HOME/inventory/mycluster"

            if [ -d "$INVENTORY_DIR" ]; then
              BACKUP_DIR="$KUBESPRAY_HOME/inventory/mycluster.backup.$(date +%s)"
              cp -r "$INVENTORY_DIR" "$BACKUP_DIR"
              echo "   âœ… Backed up existing inventory"
            fi

            mkdir -p "$INVENTORY_DIR/group_vars/all"

            # hosts.yaml (fixed: proper YAML indentation)
            cat > "$INVENTORY_DIR/hosts.yaml" << 'HOSTS'
            all:
              hosts:
                ${{ secrets.USERNAME }}:
                  ansible_connection: local
                  ansible_host: 127.0.0.1
              children:
                kube_control_plane:
                  hosts:
                    ${{ secrets.USERNAME }}:
                kube_node:
                  hosts:
                    ${{ secrets.USERNAME }}:
                etcd:
                  hosts:
                    ${{ secrets.USERNAME }}:
                k8s_cluster:
                  children:
                    kube_control_plane:
                    kube_node:
            HOSTS

            # all.yml
            cat > "$INVENTORY_DIR/group_vars/all/all.yml" << EOF
            kube_version: "1.31.6"
            container_manager: containerd
            dns_mode: coredns
            kube_pods_subnet: 10.244.0.0/16
            kube_service_addresses: 10.96.0.0/12

            # âœ… ëª…ì‹œì  API Server ì„¤ì •
            kube_apiserver_bind_address: 127.0.0.1
            kube_apiserver_advertise_address: 127.0.0.1
            apiserver_endpoint: "https://127.0.0.1:6443"

            # âœ… kubelet ì„¤ì • (DNS ëª…ì‹œ)
            kubelet_deployment_type: host
            kubelet_cgroup_driver: systemd
            kubelet_extra_args: "--cluster-dns=10.96.0.10 --cluster-domain=cluster.local --cgroup-driver=systemd"
            kubelet_cluster_dns:
              - 10.96.0.10

            # âœ… Pause image ì„¤ì •
            sandbox_image_repo: registry.k8s.io
            sandbox_image_tag: "3.10"

            # âœ… etcd ì„¤ì •
            etcd_deployment_type: host
            etcd_extra_args:
              - --listen-client-urls=http://127.0.0.1:2379
              - --advertise-client-urls=http://127.0.0.1:2379

            # âœ… containerd ì„¤ì •
            containerd_cgroup_driver: systemd
            containerd_extra_runtime_args: []

            # âœ… kubeadm íƒ€ìž„ì•„ì›ƒ
            kubeadm_init_timeout: 1800s

            # âœ… CNI
            kube_network_plugin: calico
            calico_version: 3.30.0

            # âœ… ì¸ì¦ì„œ SAN
            apiserver_cert_extra_sans:
              - localhost
              - 127.0.0.1
              - "{{ inventory_hostname }}"
              - kubernetes
              - kubernetes.default
              - kubernetes.default.svc
              - kubernetes.default.svc.cluster.local
            EOF

            echo "   âœ… Inventory configured"

            # 6ï¸âƒ£ Run Kubespray
            echo ""
            echo "ðŸš€ Running Kubespray as root (15-20 minutes)..."

            cd "$KUBESPRAY_HOME"

            ansible-playbook \
              -i "$INVENTORY_DIR/hosts.yaml" \
              --become \
              --become-method=sudo \
              -e "kubeconfig_localhost=false" \
              -e "kubeconfig_server=https://${{ secrets.HOST_IP }}:6443" \
              -e "kube_owner=${{ secrets.USERNAME }}" \
              -v \
              cluster.yml

            if [ $? -ne 0 ]; then
              echo "âŒ Kubespray failed"
              exit 1
            fi

            echo "âœ… Kubespray completed"

            # 7ï¸âƒ£ Setup kubeconfig (fixed: proper sudo for root-owned files)
            echo ""
            echo "ðŸ”‘ Setting up kubeconfig..."

            KUBECONFIG_PATH="$HOME/.kube/config"

            mkdir -p "$(dirname "$KUBECONFIG_PATH")"

            # Copy root-owned file with sudo
            sudo cp /etc/kubernetes/admin.conf "$HOME/.kube/admin.conf.tmp"
            sudo chown $(whoami):$(whoami) "$HOME/.kube/admin.conf.tmp"

            # Patch server address
            sed -i "s|server:.*6443|server: https://$HOST_IP:6443|g" "$HOME/.kube/admin.conf.tmp"

            # Move to final location
            mv "$HOME/.kube/admin.conf.tmp" "$KUBECONFIG_PATH"

            sudo chown $(whoami):$(whoami) "$KUBECONFIG_PATH"
            chmod 600 "$KUBECONFIG_PATH"

            echo "   âœ… kubeconfig ready"

            # 8ï¸âƒ£ Verify cluster
            echo ""
            echo "âœ“ Verifying cluster..."

            export KUBECONFIG="$KUBECONFIG_PATH"

            kubectl cluster-info 2>/dev/null || echo "Cluster initializing..."
            echo ""
            kubectl get nodes || true
            echo ""
            kubectl get pods -n kube-system --no-headers 2>/dev/null | head -5 || true

            echo ""
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo "ðŸŽ‰ Setup Phase Complete!"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      # Phase 3: Verification (FIRST_USERNAME)
      # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
      - name: Post-Setup Verification
        if: always()
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.PROXMOX_VM_TAILSCALE_IP }}
          username: ${{ secrets.FIRST_USERNAME }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          script: |
            set -e

            echo "âœ“ Post-Setup Verification"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

            USERNAME="${{ secrets.USERNAME }}"

            # Check shells
            echo "ðŸ” Checking user shells..."
            ROOT_SHELL=$(grep ^root /etc/passwd | cut -d: -f7)
            USER_SHELL=$(grep ^$USERNAME /etc/passwd | cut -d: -f7)

            if [[ "$ROOT_SHELL" == *"nologin"* ]]; then
              sudo usermod -s /bin/bash ${{ secrets.FIRST_USERNAME }}
            fi

            if [[ "$USER_SHELL" == *"nologin"* ]]; then
              sudo usermod -s /bin/bash "$USERNAME"
            fi

            echo "   root: $ROOT_SHELL"
            echo "   $USERNAME: $USER_SHELL"

            # Check ownership
            echo ""
            echo "ðŸ“‚ File ownership:"
            echo "   /opt/kubespray: $(ls -ld /opt/kubespray 2>/dev/null | awk '{print $3, $4}')"
            echo "   /opt/venv-kubespray: $(ls -ld /opt/venv-kubespray 2>/dev/null | awk '{print $3, $4}')"

            # Check K8s
            echo ""
            echo "ðŸ“Š Kubernetes Status:"
            sudo -u "$USERNAME" -H bash << 'K8S_CHECK'
            export KUBECONFIG="$HOME/.kube/config"
            echo "   Nodes: $(kubectl get nodes --no-headers 2>/dev/null | wc -l)"
            echo "   Pods: $(kubectl get pods -n kube-system --no-headers 2>/dev/null | wc -l)"
            K8S_CHECK

            echo ""
            echo "âœ… Verification complete"
