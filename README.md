# haema's Home Lab

## ğŸ“Š Project Roadmap & Variance Analysis (Updated: 2026-02-06)

| Stage                        | Planned         | Actual               | Variance | Delay Analysis & Action Items                                      |
| :--------------------------- | :-------------- | :------------------- | :------: | :----------------------------------------------------------------- |
| **01. System Design**        | Jan 16 - Jan 19 | Jan 16 - Jan 19 |    0d    | **[Complete]** Architecture finalized for fixed hardware baseline.   |
| **02. DevOps & Environment** | Jan 23 - Jan 24 | Jan 26 - Jan 29 |    5d    | **[Cause & Complete]** Changes to Kubernetes via Kubespray           |
| **03. Model Evaluation** | Jan 30 - Jan 31 | Jan 30 - Feb 05 | 5d | **[Cause & Complete]** iGPU passthrough (c5:00.0)<br>**[Complete]** Qwen2.5 + bge models selected |
| **04. Development** | Feb 07 - Feb 27 | Feb 07 - **Ongoing** | - | - |

> **Current Status:** ğŸ—ï¸ Stage 04 (Development) in progress.

---

## ğŸ›ï¸ System Architecture

![System Architecture Diagram](./system_architecture4.png)

## âš™ï¸ Infrastructure Constraints & Design Decisions

**Hardware Topology**:

- **Compute Node (A)**: 6-core CPU, 12 threads, 32GB RAM (Kubernetes control plane + application workloads)
- **Inference Node (B)**: 8-core CPU, 16 threads, 64GB RAM, iGPU
  (Model serving + local LLM inference)
- **Constraint**: Both nodes are fixed in specs; no cloud spillover for training. 

**Cost-Sensitive Engineering**:

- **Local Processing**: sLLM handles summarization and RAG embeddings
- **API Generation**: Gemini API called only for final text generation
- **Result**: Significant cost savings vs. 100% cloud inference

## ğŸ› ï¸ Technology Stack & Tooling

- **Virtualization**: Proxmox VE (iGPU Passthrough)
- **Orchestration**: Kubernetes via Kubespray/Ansible
- **Network & Access**: Tailscale (Mesh VPN)
- **CI/CD & GitOps**: GitHub Actions, ArgoCD
- **Monitoring**: Proxmox & ArgoCD & Discord Webhook
- **MLOps Pipeline**: Apache Airflow (Batch) + Ollama + FastAPI
- **Model**: BGE-M3, EXAONE-3.5:7.8B, Qwen2.5:14B, Gemini 2.5 Pro (API)
- **Langauge & Framework**: Python, Django, FastAPI, Kotlin, Spring Boot
- **Database**: Redis (Caching), PostgreSQL + PGVector

## ğŸ“ Folder Architecture
```bash
repo/
  â”œâ”€â”€ .github/workflows/
  â”‚             â”œâ”€â”€ bootstrap.yaml           # Kubespray â†’ k8s + Ansible
  â”‚             â”œâ”€â”€ argocd-setup.yaml        # ArgoCD
  â”‚             â”œâ”€â”€ deploy-gateway.yml
  â”‚             â”œâ”€â”€ deploy-frontend.yml
  â”‚             â”œâ”€â”€ deploy-backend.yml
  â”‚             â””â”€â”€ deploy-models.yml
  â”‚
  â”œâ”€â”€ apps/
  â”‚     â”œâ”€â”€ gateway/    # Kotlin + Spring
  â”‚     â”œâ”€â”€ frontend/   # Typescript + React
  â”‚     â”œâ”€â”€ backend/    # Python + Django
  â”‚     â””â”€â”€ models/     # Python + FastAPI
  â”‚
  â”œâ”€â”€ argocd/
  â”‚     â”œâ”€â”€ gateway.yaml
  â”‚     â”œâ”€â”€ frontend.yaml
  â”‚     â””â”€â”€ root-app.yaml
  â”‚
  â”œâ”€â”€ manifests/
  â”‚     â”œâ”€â”€ gateway/          # Kotlin + Spring
  â”‚     â”œâ”€â”€ frontend/         # Typescript + React
  â”‚     â”œâ”€â”€ backend/          # Python + Django
  â”‚     â””â”€â”€ models/           # Python + FastAPI
  â”‚
  â””â”€â”€ README.md
```

## ğŸ¯ Key Challenges & Tasks

1. **Design and visualize** the overall service architecture to optimize limited local resources.
2. **Establish** a robust DevOps pipeline and environment setup for automated deployment.
